import os
import sys
import json
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# Configuration
load_dotenv()

def initialize_llm():
    """Initialize the LLM for evaluation."""
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL")
    model = os.getenv("LLM_MODEL", "gpt-3.5-turbo")
    
    if not api_key:
        raise ValueError("OPENAI_API_KEY not found in environment variables")
    
    # Fixed: Changed openai_api_base to base_url
    kwargs = {
        "openai_api_key": api_key,
        "model_name": model,
        "temperature": 0.3  # Lower temperature for more consistent evaluations
    }
    
    # Only add base_url if it's provided
    if base_url:
        kwargs["base_url"] = base_url
    
    llm = ChatOpenAI(**kwargs)
    
    return llm

def evaluate_answer(user_question, system_answer, chunks_related):
    """
    Evaluate the quality of a RAG system answer.
    
    Args:
        user_question: The original question asked by the user
        system_answer: The answer generated by the system
        chunks_related: List of chunks used to generate the answer
    
    Returns:
        Dictionary with score (0-10) and reason
    """
    llm = initialize_llm()
    
    # Format chunks for the evaluation prompt
    chunks_text = "\n\n".join([
        f"Chunk {i+1}: {chunk.get('text', chunk) if isinstance(chunk, dict) else chunk}"
        for i, chunk in enumerate(chunks_related)
    ])
    
    evaluation_prompt = f"""You are an expert evaluator for RAG (Retrieval-Augmented Generation) systems. Your task is to score the quality of an answer on a scale of 0-10 based on specific criteria.

User Question:
{user_question}

Retrieved Chunks:
{chunks_text}

System Answer:
{system_answer}

Evaluation Criteria:
1. Chunk Relevance (0-3 points): Are the retrieved chunks relevant to the question?
2. Answer Accuracy (0-3 points): Does the answer accurately reflect the information in the chunks?
3. Completeness (0-2 points): Does the answer fully address the question?
4. Clarity (0-2 points): Is the answer clear, coherent, and well-structured?

Provide your evaluation in JSON format with two fields:
- "score": An integer from 0 to 10
- "reason": A brief explanation of the score covering each criterion

Example format:
{{"score": 8, "reason": "Chunk Relevance (3/3): All chunks directly address PTO policy. Accuracy (2/3): Answer correctly cites the policy but misses rollover details. Completeness (2/2): Fully answers the question. Clarity (1/2): Slightly wordy but understandable."}}

Your evaluation:"""

    try:
        response = llm.invoke([HumanMessage(content=evaluation_prompt)])
        content = response.content.strip()
        
        # Try to extract JSON from the response
        # Sometimes the model wraps it in markdown code blocks
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        result = json.loads(content)
        
        # Validate the result
        if "score" not in result or "reason" not in result:
            raise ValueError("Response missing required fields")
        
        # Ensure score is in valid range
        result["score"] = max(0, min(10, int(result["score"])))
        
        return result
        
    except json.JSONDecodeError:
        # If JSON parsing fails, try to extract score and use full response as reason
        return {
            "score": 5,
            "reason": f"Evaluation completed but response format was unexpected. Raw response: {content}"
        }
    except Exception as e:
        return {
            "score": 0,
            "reason": f"Evaluation failed: {str(e)}"
        }

def main():
    """Main entry point for the evaluator."""
    try:
        # Check if input was provided
        if len(sys.argv) < 2:
            print("Usage: python src/evaluator.py <query_result_json>")
            print("\nExample:")
            print('python src/evaluator.py \'{"user_question": "...", "system_answer": "...", "chunks_related": [...]}\'')
            print("\nOr use with query output:")
            print('python src/query.py "question" | python src/evaluator.py -')
            sys.exit(1)
        
        # Read input
        if sys.argv[1] == '-':
            # Read from stdin
            input_data = sys.stdin.read()
        else:
            # Read from command line argument
            input_data = sys.argv[1]
        
        # Parse the query result
        query_result = json.loads(input_data)
        
        # Validate required fields
        required_fields = ["user_question", "system_answer", "chunks_related"]
        for field in required_fields:
            if field not in query_result:
                raise ValueError(f"Missing required field: {field}")
        
        # Evaluate the answer
        print("Evaluating answer quality...", file=sys.stderr)
        evaluation = evaluate_answer(
            query_result["user_question"],
            query_result["system_answer"],
            query_result["chunks_related"]
        )
        
        # Add evaluation to the original result
        query_result["evaluation"] = evaluation
        
        # Output the complete result with evaluation
        print(json.dumps(query_result, indent=2))
        
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON input - {e}", file=sys.stderr)
        sys.exit(1)
    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"Unexpected error: {e}", file=sys.stderr)
        raise

if __name__ == "__main__":
    main()
